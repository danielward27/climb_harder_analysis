{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Climbing has relatively recently picked up in popularity, being included for the first time in the Tokyo 2020 olympics. There is very little data on climbing habits of successful (and unsucessful) climbers, and training routines are generally heavily anecdotal. A reddit user, u/higiff, posted a survey on the \"climb harder\" subreddit (https://www.reddit.com/r/climbharder/comments/6693ua/climbharder_survey_results/), in order to question climbers about their training habits. This to my knowledge is the largest publically available dataset on climbers and their training strategies.\n",
    "\n",
    "Having been climbing consistently for around four years now, I was intrigued to see what information this dataset might contain. Causal inference is unfortunately impossible with this dataset, but nevertheless, in the absence of better data, understanding the habits of highly effective climbers is still potentially useful.\n",
    "\n",
    "I have cleaned the data and extracted features in the notebook data_cleaning.ipynb. Here, I will deal with missing values and build a model to try and predict `max_boulder_grade` from features such as training strategies, height, sex etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "All imports are included here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data imputation\n",
    "The remaining missing data seems to be missing completely at random.\n",
    "\n",
    "I'll try two strategies and see which offers the best results:\n",
    "\n",
    "**1. No data imputation by dropping rows/columns with `np.nan` :**\n",
    "- Advantage: Avoids potential bias from imputing data. Gives a good benchmark.\n",
    "- Disadvantage: Less training data, loss of power.\n",
    "\n",
    "**2. Imputing missing values in `X_train` and `X_test`:**\n",
    "- Advantage: Potentially more power as can utilise the train set more fully.\n",
    "- Disadvantage: Imputation can often bias the model and make it perform worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arm_span_cm has 33 missing values\n",
      "max_boulder_grade has 17 missing values\n",
      "max_pull_ups has 120 missing values\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "for col in list(df):\n",
    "    missing = df[col].isnull().sum()\n",
    "    if missing > 0:\n",
    "        print(\"{} has {} missing values\".format(col, missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. No data imputation by dropping rows/columns with `np.nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has shape: (520, 49)\n",
      "0 np.nan values in dataset\n"
     ]
    }
   ],
   "source": [
    "df_drop_na = df.copy()\n",
    "df_drop_na = df_drop_na.drop(columns=[\"max_pull_ups\", \"arm_span_cm\"]) #  Drop cols with lots of np.nans\n",
    "df_drop_na = df_drop_na.dropna() #  Drop remaining rows with nas\n",
    "\n",
    "print(\"Dataset has shape: {}\".format(df_drop_na.shape))\n",
    "print(\"{} np.nan values in dataset\".format(df_drop_na.isna().values.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll train a random forest regressor, and use 5-fold cross validation and assess the model fit with the R^2 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.501 +/- 0.171\n"
     ]
    }
   ],
   "source": [
    "X = df_drop_na.drop(columns=\"max_boulder_grade\")\n",
    "y = df_drop_na[\"max_boulder_grade\"]\n",
    "rf = RandomForestRegressor()  # Use defualts for now\n",
    "scores = cross_val_score(rf, X, y, cv=5, scoring=\"r2\")\n",
    "print(\"R^2: {:.3f} +/- {:.3f}\".format(scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Imputing np.nan in `X_train` and `X_test`:\n",
    "Let's see if we can improve this R^2 score by imputing the missing values instead. I'll use the `IterarativeImputer()` function from scikit-learn. This will (by defualt) impute values sequentially using Bayesian ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(520, 51)\n"
     ]
    }
   ],
   "source": [
    "df_impute = df.copy()\n",
    "df_impute = df_impute[df_impute['max_boulder_grade'].notna()] # Keep all na rows except in our dependant variable\n",
    "print(df_impute.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.531 +/- 0.152\n"
     ]
    }
   ],
   "source": [
    "X = df_impute.drop(columns = \"max_boulder_grade\")\n",
    "y = df_impute[\"max_boulder_grade\"]\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"imputer\", IterativeImputer(max_iter=100)),\n",
    "    (\"rf\", RandomForestRegressor())\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring=\"r2\")\n",
    "print(\"R^2: {:.3f} +/- {:.3}\".format(scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation increased the R^2 value by ~0.3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for hyperparmater optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "R2: 0.533 +/- 0.15\n",
      "\n",
      "Achieved with hyperparameters:\n",
      "{'imputer__initial_strategy': 'median', 'rf__n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "X = df_impute.drop(columns=\"max_boulder_grade\")\n",
    "y = df_impute[\"max_boulder_grade\"]\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"imputer\", IterativeImputer(max_iter=100)),\n",
    "    (\"rf\", RandomForestRegressor())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"imputer__initial_strategy\": [\"mean\", \"median\"],\n",
    "    \"rf__n_estimators\": [100, 500, 1000],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=5, param_grid=param_grid, scoring=\"r2\")\n",
    "grid.fit(X, y)\n",
    "\n",
    "std = grid.cv_results_[\"std_test_score\"][grid.cv_results_[\"rank_test_score\"]==1][0]\n",
    "print(\"Best score:\")\n",
    "print(\"R2: {:.3f} +/- {:.3}\\n\".format(grid.best_score_, std*2))\n",
    "print(\"Achieved with hyperparameters:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_model.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(grid.best_estimator_, 'rf_model.joblib', compress = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do\n",
    "- Add a proper EDA.\n",
    "- Try other regressors and compare performance.\n",
    "- Look at importances of features in the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
